{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/douglasbouchet/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "import nltk\n",
    "import numpy as np\n",
    "from nlp_helper import *\n",
    "from nltk import pos_tag\n",
    "from gensim import models\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Phrases\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59794\n"
     ]
    }
   ],
   "source": [
    "df = read_moviedb_data()\n",
    "df_plots = df.copy()\n",
    "# keep only the overview and providers columns as we don't use others for NLP\n",
    "df_plots = df_plots[[\"overview\", \"providers\"]]\n",
    "# replace nan for overview by \"\" and nan for provider by {}\n",
    "df_plots[\"providers\"] = df_plots[\"providers\"].fillna(\"{}\")\n",
    "df_plots[\"overview\"] = df_plots[\"overview\"].fillna(\"\")\n",
    "df_plots.head()\n",
    "print(len(df_plots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot preparation\n",
    "\n",
    "We will transform the plots in order to make them intepretable by an LDA model. This includes\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "- Removing of stopwords\n",
    "\n",
    "This is usefull as we want to find ressemblance between words, so we should replace words with same meaning by one \n",
    "common word.  \n",
    "We also want to remove most commun words. This allows to remove low-information words, allowing our \n",
    "model to focus on important $\\\\$ words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overview</th>\n",
       "      <th>providers</th>\n",
       "      <th>tokenized_plots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The adventures of a female reporter in the 1890s.</td>\n",
       "      <td>{}</td>\n",
       "      <td>[The, adventures, of, a, female, reporter, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just as Galeen and Wegener's Der Golem (1915) ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Just, as, Galeen, and, Wegener, 's, Der, Gole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first feature-length motion picture produc...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[The, first, feature-length, motion, picture, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australian bushranger movie.  The first filmed...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Australian, bushranger, movie, ., The, first,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L. Frank Baum would appear in a white suit and...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[L., Frank, Baum, would, appear, in, a, white,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            overview providers  \\\n",
       "0  The adventures of a female reporter in the 1890s.        {}   \n",
       "1  Just as Galeen and Wegener's Der Golem (1915) ...        {}   \n",
       "2  The first feature-length motion picture produc...        {}   \n",
       "3  Australian bushranger movie.  The first filmed...        {}   \n",
       "4  L. Frank Baum would appear in a white suit and...        {}   \n",
       "\n",
       "                                     tokenized_plots  \n",
       "0  [The, adventures, of, a, female, reporter, in,...  \n",
       "1  [Just, as, Galeen, and, Wegener, 's, Der, Gole...  \n",
       "2  [The, first, feature-length, motion, picture, ...  \n",
       "3  [Australian, bushranger, movie, ., The, first,...  \n",
       "4  [L., Frank, Baum, would, appear, in, a, white,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the plots\n",
    "df_plots['tokenized_plots'] = df_plots['overview'].apply(\n",
    "    lambda movie_plot: word_tokenize(movie_plot))\n",
    "df_plots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "we start by assocating a POS tag to each word (i.e if a word is a Noun, Verb, Adjective, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(The, DT), (adventures, NNS), (of, IN), (a, D...\n",
       "1    [(Just, RB), (as, IN), (Galeen, NNP), (and, CC...\n",
       "2    [(The, DT), (first, JJ), (feature-length, JJ),...\n",
       "3    [(Australian, JJ), (bushranger, NN), (movie, N...\n",
       "4    [(L., NNP), (Frank, NNP), (Baum, NNP), (would,...\n",
       "Name: plots_with_POS_tag, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plots['plots_with_POS_tag'] = df_plots['tokenized_plots'].apply(\n",
    "    lambda tokenized_plot: pos_tag(tokenized_plot))\n",
    "df_plots['plots_with_POS_tag'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word has no tag we don't change it. However if there is a tag, we lemmatize the word according to its tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'adventure',\n",
       " 'of',\n",
       " 'a',\n",
       " 'female',\n",
       " 'reporter',\n",
       " 'in',\n",
       " 'the',\n",
       " '1890s',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# Now we can lemmatize each word, given its POS tag\n",
    "df_plots['lemmatized_plots'] = df_plots['plots_with_POS_tag'].apply(\n",
    "    lambda tokenized_plot: [word[0] if get_wordnet_pos(word[1]) == ''\\\n",
    "        else lemmatizer.lemmatize(word[0], get_wordnet_pos(word[1])) for word in tokenized_plot])\n",
    "df_plots['lemmatized_plots'].head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â TODO list of stop words may be improved\n",
    "# create our list of stopwords\n",
    "stop_words = ['\\'s']\n",
    "all_stopwords = stopwords.words('English') + list(string.punctuation) + stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        [adventure, female, reporter]\n",
       "1    [galeen, wegener, der, golem, see, testament, ...\n",
       "Name: plots_without_stopwords, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# remove the white space inside each words\n",
    "df_plots['plots_without_stopwords'] = df_plots['lemmatized_plots'].apply(\n",
    "    lambda tokenized_plot: [word.strip() for word in tokenized_plot])\n",
    "# lowercase all words in each plot\n",
    "df_plots['plots_without_stopwords'] = df_plots['plots_without_stopwords'].apply(\n",
    "    lambda plot: [word.lower() for word in plot])\n",
    "# remove stopwords from the plots\n",
    "df_plots['plots_without_stopwords'] = df_plots['plots_without_stopwords'].apply(\n",
    "    lambda plot: [word for word in plot if word not in all_stopwords])\n",
    "# remove word if contains other letter than a-z or is a single character\n",
    "df_plots['plots_without_stopwords'] = df_plots['plots_without_stopwords'].apply(\n",
    "    lambda plot: [word for word in plot if word.isalpha() and len(word) > 1])\n",
    "df_plots['plots_without_stopwords'].head()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We kept 48.0% of the words in the corpus\n"
     ]
    }
   ],
   "source": [
    "before_stop_words_total_number_of_words =\\\n",
    "     len([word for sentence in df_plots['lemmatized_plots'] for word in sentence])\n",
    "after_stop_words_total_number_of_words =\\\n",
    "     len([word for sentence in df_plots['plots_without_stopwords'] for word in sentence])\n",
    "print(\"We kept {}% of the words in the corpus\".format(\\\n",
    "    round(after_stop_words_total_number_of_words/before_stop_words_total_number_of_words, 2) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Direchlet Allocation\n",
    "\n",
    "We need to create a list of tokens, i.e words that will be used inside our dictionary (depending on their frequency). \n",
    "$\\\\$\n",
    "We can start by creating bi-gram for some words (represent to one words by one unique composed word)  \n",
    "It can be also interesting to see if creating tri-gram allows to extract more information from plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['adventure', 'female_reporter'], ['galeen', 'wegener', 'der', 'golem', 'see', 'testament', 'early', 'german', 'film', 'artistry', 'story', 'kelly', 'gang', 'symbolize', 'birth', 'australian', 'film_industry', 'emergence', 'australian', 'cinema', 'identity', 'even', 'significantly', 'herald', 'emergence', 'feature_film', 'format', 'however', 'fragment', 'original', 'production', 'one', 'hour', 'know_exist', 'preserve', 'national', 'film', 'sound', 'archive', 'canberra', 'efforts', 'reconstruction', 'make', 'film', 'available', 'modern', 'audience']]\n"
     ]
    }
   ],
   "source": [
    "tokens = df_plots['plots_without_stopwords'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "tokens = list(bigram_model[tokens])\n",
    "print(tokens[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_below = 60 # minimum number of documents a word must be present in to be kept\n",
    "no_above = 0.5 # maximum proportion of documents a word can be present in to be kept\n",
    "n_topics = 10 # number of topics\n",
    "n_passes = 10 # number of passes through the corpus during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnary & Corpus\n",
    "\n",
    "The dictionnary will be the list of unique words, and the corpus a list of movie plots bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 13475\n",
      "Dictionary first 10 elements: [(0, 'adventure'), (1, 'female_reporter'), (2, 'archive'), (3, 'audience'), (4, 'australian'), (5, 'available'), (6, 'birth'), (7, 'cinema'), (8, 'der'), (9, 'early')]\n",
      "Corpus size: 59794\n",
      "Corpus first 2 elements: [[(0, 1), (1, 1)], [(2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 3), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# we create a dictionary that maps each word to a unique integer\n",
    "# we also create a corpus. Each movie plot is encoded as a bag of words in the corpus. \n",
    "# A bag of word means that we count the number of times each word appears in the mvoie plot\n",
    "dictionary,corpus = build_dictionnary_and_corpus(tokens, no_below=no_below, no_above=no_above)\n",
    "print(\"Dictionary size: {}\".format(len(dictionary)))\n",
    "print(\"Dictionary first 10 elements: {}\".format(list(dictionary.items())[0:10]))\n",
    "print(\"Corpus size: {}\".format(len(corpus)))\n",
    "print(\"Corpus first 2 elements: {}\".format(corpus[0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9999)\n",
    "lda_model = create_lda_model(corpus, dictionary, num_topics=n_topics, passes=n_passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: keep include french past government several actor appear plot world_war course best share\n",
      "Topic 1: meet decide cop evil happen body affair high_school earth sell living camp hero\n",
      "Topic 2: play learn save team visit director little different four future late game hide\n",
      "Topic 3: life get woman man family father become work new live time son leave\n",
      "Topic 4: find take go friend try girl help child way return however force must\n",
      "Topic 5: one two young wife love kill old world daughter brother also people set\n",
      "Topic 6: use dream bring village another whose company boyfriend great survive hand music without\n",
      "Topic 7: film make story want first back look movie show star well long much\n",
      "Topic 8: turn group murder three know escape local police order hong_kong gang student give\n",
      "Topic 9: fight name still soldier island battle street experience teacher documentary land human free\n"
     ]
    }
   ],
   "source": [
    "# get the topics \n",
    "topics = get_topics(lda_model, num_topics=n_topics, num_words=10)\n",
    "# print topics with new line\n",
    "for i,topic in enumerate(topics):\n",
    "    print(\"Topic {}: {}\".format(i,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0.04638879), (1, 0.06420414), (2, 0.07004631), (3, 0.19013405), (4, 0.14304884), (5, 0.15949595), (6, 0.06626958), (7, 0.08734115), (8, 0.120726734), (9, 0.0523445)], [(0, 0.030123878), (1, 0.04168074), (2, 0.056213398), (3, 0.12342403), (4, 0.12507316), (5, 0.11432252), (6, 0.22635376), (7, 0.14898047), (8, 0.08915698), (9, 0.04467106)], [(0, 0.036190297), (1, 0.05009002), (2, 0.093504086), (3, 0.17425533), (4, 0.111605875), (5, 0.11148201), (6, 0.14201573), (7, 0.13288711), (8, 0.09418971), (9, 0.05377984)]]\n"
     ]
    }
   ],
   "source": [
    "# for each movie plot, get its topic distribution (i.e the probability of each topic) in descending order\n",
    "topic_distributions = get_topic_distribution(lda_model, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd36d5f1320431a45e01240dcc8ffb6ab490925b08bbe6d8a89832130106a873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
