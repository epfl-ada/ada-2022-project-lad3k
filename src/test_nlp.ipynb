{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/douglasbouchet/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "import nltk\n",
    "import numpy as np\n",
    "from nlp_helper import *\n",
    "from nltk import pos_tag\n",
    "from gensim import models\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Phrases\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_moviedb_data()\n",
    "df_plots = df.copy()\n",
    "# keep only the overview and providers columns as we don't use others for NLP\n",
    "df_plots = df_plots[[\"overview\", \"providers\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview Analysis\n",
    "\n",
    "In this NLP exploration, we are mostly interested by the overview and providers fields.  \n",
    "Let's see if some movies don't contain overviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7 % of movies have no overview\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    The adventures of a female reporter in the 1890s.\n",
       "1    Just as Galeen and Wegener's Der Golem (1915) ...\n",
       "2    The first feature-length motion picture produc...\n",
       "3    Australian bushranger movie.  The first filmed...\n",
       "4    L. Frank Baum would appear in a white suit and...\n",
       "5                             Know what this is about?\n",
       "6                                                     \n",
       "7                             Know what this is about?\n",
       "Name: overview, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Â print the percentage of movies with no overview\n",
    "nb_with_no_overview = len(df_plots[df_plots['overview'].isnull()])\n",
    "print(round(nb_with_no_overview / len(df_plots) * 100, 1), \"% of movies have no overview\")\n",
    "\n",
    "# replace the missing values with an empty string\n",
    "df_plots['overview'] = df_plots['overview'].fillna('')\n",
    "\n",
    "df_plots.head(8)['overview']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that some movies (5,7) contain a non empty overview, but which indicate that there is no overview for this movie.  \n",
    "We can replace them by empty overviews. However this replacement may not be exhaustive if some useless plots are not\n",
    "$\\\\$ \"Know what this is about?\" but something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % of movies contain \"Know what this is about?\" as an overview\n"
     ]
    }
   ],
   "source": [
    "# compute percentage of movies with overview \"Know what this is about\"\n",
    "print(round(len(df_plots[df_plots['overview'].str.contains('about')]['overview'])/len(df_plots),2)*100,\\\n",
    "     \"% of movies contain \\\"Know what this is about?\\\" as an overview\")\n",
    "# we can replace them with an empty string\n",
    "df_plots['overview'] = df_plots['overview'].str.replace('Know what this is about?','', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, we have roughly 20% of movies without overview. This is tolerable given that our dataset is large, but firstly our search is not $\\\\$ exhaustive, and secondly we will have to check that most of the movies on the streaming platforms have an overview to be able to $\\\\$ apply NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to previous analysis, we decided to use providers of Switzerland and US, we will now see if the movies\n",
    "provided in these countries possess enough plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by cleaning provider data\n",
    "df_plots['providers'] = df_plots['providers'].fillna('{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0 % of movies in US have plots\n",
      "97.0 % of movies in CH have plots\n"
     ]
    }
   ],
   "source": [
    "# US provider movies\n",
    "df_plots_us = df_plots[df_plots['providers'].str.contains('US')]\n",
    "# UK provider movies\n",
    "df_plots_ch = df_plots[df_plots['providers'].str.contains('CH')]\n",
    "# keep only movies whom plots isn't empty   \n",
    "df_plots_us_overview = df_plots_us[df_plots_us['overview'] != '']\n",
    "# filter the providers from CH\n",
    "df_plots_ch_overview = df_plots_ch[df_plots_ch['overview'] != '']\n",
    "\n",
    "# print ratio of movies in US having plots\n",
    "print(round(len(df_plots_us_overview)/len(df_plots_us),2)*100, \"% of movies in US have plots\")\n",
    "# print ratio of movies in CH having plots\n",
    "print(round(len(df_plots_ch_overview)/len(df_plots_ch),2)*100, \"% of movies in CH have plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that we can work on an NLP for topics analysis for the movies provided in US and CH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot preparation\n",
    "\n",
    "Now that we have seen we have enough movies plots in the US and CH regions, we can work on our topics analysis. $\\\\$\n",
    "To make a simple first exploration of topics analysis, we will simplify by merging movies from CH and US together $\\\\$\n",
    "as language is roughly the same. We could however try to split between the two in further analysis. \n",
    "\n",
    "We will transform the plots in order to make them intepretable by an LDA model. This includes\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "- Removing of stopwords\n",
    "\n",
    "This is usefull as we want to find ressemblance between words, so we should replace words with same meaning by one \n",
    "common word.  \n",
    "We also want to remove most commun words. This allows to remove low-information words, allowing our \n",
    "model to focus on important $\\\\$ words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep movie with overview and providers in US and CH\n",
    "df_plots = pd.concat([df_plots_us_overview, df_plots_ch_overview])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the plots\n",
    "df_plots['tokenized_plots'] = df_plots['overview'].apply(\n",
    "    lambda movie_plot: word_tokenize(movie_plot))\n",
    "df_plots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "we start by assocating a POS tag to each word (i.e if a word is a Noun, Verb, Adjective, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(The, DT), (adventures, NNS), (of, IN), (a, D...\n",
       "1    [(Just, RB), (as, IN), (Galeen, NNP), (and, CC...\n",
       "2    [(The, DT), (first, JJ), (feature-length, JJ),...\n",
       "3    [(Australian, JJ), (bushranger, NN), (movie, N...\n",
       "4    [(L., NNP), (Frank, NNP), (Baum, NNP), (would,...\n",
       "Name: plots_with_POS_tag, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plots['plots_with_POS_tag'] = df_plots['tokenized_plots'].apply(\n",
    "    lambda tokenized_plot: pos_tag(tokenized_plot))\n",
    "df_plots['plots_with_POS_tag'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word has no tag we don't change it. However if there is a tag, we lemmatize the word according to its tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'adventure',\n",
       " 'of',\n",
       " 'a',\n",
       " 'female',\n",
       " 'reporter',\n",
       " 'in',\n",
       " 'the',\n",
       " '1890s',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# Now we can lemmatize each word, given its POS tag\n",
    "df_plots['lemmatized_plots'] = df_plots['plots_with_POS_tag'].apply(\n",
    "    lambda tokenized_plot: [word[0] if get_wordnet_pos(word[1]) == ''\\\n",
    "        else lemmatizer.lemmatize(word[0], get_wordnet_pos(word[1])) for word in tokenized_plot])\n",
    "df_plots['lemmatized_plots'].head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â TODO list of stop words may be improved\n",
    "# create our list of stopwords\n",
    "stop_words = ['\\'s']\n",
    "all_stopwords = stopwords.words('English') + list(string.punctuation) + stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        [adventure, female, reporter]\n",
       "1    [galeen, wegener, der, golem, see, testament, ...\n",
       "Name: plots_without_stopwords, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# remove the white space inside each words\n",
    "df_plots['plots_without_stopwords'] = df_plots['lemmatized_plots'].apply(\n",
    "    lambda tokenized_plot: [word.strip() for word in tokenized_plot])\n",
    "# lowercase all words in each plot\n",
    "df_plots['plots_without_stopwords'] = df_plots['plots_without_stopwords'].apply(\n",
    "    lambda plot: [word.lower() for word in plot])\n",
    "# remove stopwords from the plots\n",
    "df_plots['plots_without_stopwords'] = df_plots['plots_without_stopwords'].apply(\n",
    "    lambda plot: [word for word in plot if word not in all_stopwords])\n",
    "# remove word if contains other letter than a-z or is a single character\n",
    "df_plots['plots_without_stopwords'] = df_plots['plots_without_stopwords'].apply(\n",
    "    lambda plot: [word for word in plot if word.isalpha() and len(word) > 1])\n",
    "df_plots['plots_without_stopwords'].head()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We kept 48.0% of the words in the corpus\n"
     ]
    }
   ],
   "source": [
    "before_stop_words_total_number_of_words =\\\n",
    "     len([word for sentence in df_plots['lemmatized_plots'] for word in sentence])\n",
    "after_stop_words_total_number_of_words =\\\n",
    "     len([word for sentence in df_plots['plots_without_stopwords'] for word in sentence])\n",
    "print(\"We kept {}% of the words in the corpus\".format(\\\n",
    "    round(after_stop_words_total_number_of_words/before_stop_words_total_number_of_words, 2) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Direchlet Allocation\n",
    "\n",
    "We need to create a list of tokens, i.e words that will be used inside our dictionary (depending on their frequency). \n",
    "$\\\\$\n",
    "We can start by creating bi-gram for some words (represent to one words by one unique composed word)  \n",
    "It can be also interesting to see if creating tri-gram allows to extract more information from plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['adventure', 'female_reporter'], ['galeen', 'wegener', 'der', 'golem', 'see', 'testament', 'early', 'german', 'film', 'artistry', 'story', 'kelly', 'gang', 'symbolize', 'birth', 'australian', 'film_industry', 'emergence', 'australian', 'cinema', 'identity', 'even', 'significantly', 'herald', 'emergence', 'feature_film', 'format', 'however', 'fragment', 'original', 'production', 'one', 'hour', 'know_exist', 'preserve', 'national', 'film', 'sound', 'archive', 'canberra', 'efforts', 'reconstruction', 'make', 'film', 'available', 'modern', 'audience']]\n"
     ]
    }
   ],
   "source": [
    "tokens = df_plots['plots_without_stopwords'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "tokens = list(bigram_model[tokens])\n",
    "print(tokens[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_below = 60 # minimum number of documents a word must be present in to be kept\n",
    "no_above = 0.5 # maximum proportion of documents a word can be present in to be kept\n",
    "n_topics = 10 # number of topics\n",
    "n_passes = 10 # number of passes through the corpus during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnary & Corpus\n",
    "\n",
    "The dictionnary will be the list of unique words, and the corpus a list of movie plots bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 13475\n",
      "Dictionary first 10 elements: [(0, 'adventure'), (1, 'female_reporter'), (2, 'archive'), (3, 'audience'), (4, 'australian'), (5, 'available'), (6, 'birth'), (7, 'cinema'), (8, 'der'), (9, 'early')]\n",
      "Corpus size: 59794\n",
      "Corpus first 2 elements: [[(0, 1), (1, 1)], [(2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 3), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# we create a dictionary that maps each word to a unique integer\n",
    "# we also create a corpus. Each movie plot is encoded as a bag of words in the corpus. \n",
    "# A bag of word means that we count the number of times each word appears in the mvoie plot\n",
    "dictionary,corpus = build_dictionnary_and_corpus(tokens, no_below=no_below, no_above=no_above)\n",
    "print(\"Dictionary size: {}\".format(len(dictionary)))\n",
    "print(\"Dictionary first 10 elements: {}\".format(list(dictionary.items())[0:10]))\n",
    "print(\"Corpus size: {}\".format(len(corpus)))\n",
    "print(\"Corpus first 2 elements: {}\".format(corpus[0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9999)\n",
    "lda_model = create_lda_model(corpus, dictionary, num_topics=n_topics, passes=n_passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: keep include french past government several actor appear plot world_war course best share\n",
      "Topic 1: meet decide cop evil happen body affair high_school earth sell living camp hero\n",
      "Topic 2: play learn save team visit director little different four future late game hide\n",
      "Topic 3: life get woman man family father become work new live time son leave\n",
      "Topic 4: find take go friend try girl help child way return however force must\n",
      "Topic 5: one two young wife love kill old world daughter brother also people set\n",
      "Topic 6: use dream bring village another whose company boyfriend great survive hand music without\n",
      "Topic 7: film make story want first back look movie show star well long much\n",
      "Topic 8: turn group murder three know escape local police order hong_kong gang student give\n",
      "Topic 9: fight name still soldier island battle street experience teacher documentary land human free\n"
     ]
    }
   ],
   "source": [
    "# get the topics \n",
    "topics = get_topics(lda_model, num_topics=n_topics, num_words=10)\n",
    "# print topics with new line\n",
    "for i,topic in enumerate(topics):\n",
    "    print(\"Topic {}: {}\".format(i,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0.04638879), (1, 0.06420414), (2, 0.07004631), (3, 0.19013405), (4, 0.14304884), (5, 0.15949595), (6, 0.06626958), (7, 0.08734115), (8, 0.120726734), (9, 0.0523445)], [(0, 0.030123878), (1, 0.04168074), (2, 0.056213398), (3, 0.12342403), (4, 0.12507316), (5, 0.11432252), (6, 0.22635376), (7, 0.14898047), (8, 0.08915698), (9, 0.04467106)], [(0, 0.036190297), (1, 0.05009002), (2, 0.093504086), (3, 0.17425533), (4, 0.111605875), (5, 0.11148201), (6, 0.14201573), (7, 0.13288711), (8, 0.09418971), (9, 0.05377984)]]\n"
     ]
    }
   ],
   "source": [
    "# for each movie plot, get its topic distribution (i.e the probability of each topic) in descending order\n",
    "topic_distributions = get_topic_distribution(lda_model, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd36d5f1320431a45e01240dcc8ffb6ab490925b08bbe6d8a89832130106a873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
